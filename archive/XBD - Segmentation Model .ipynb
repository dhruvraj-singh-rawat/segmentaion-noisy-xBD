{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df632e2a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bd098aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/1js5b25s1b1dk8g7drn7_g8h0000gn/T/ipykernel_16143/4065838880.py:14: DeprecationWarning: Please use `generic_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import generic_filter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import segmentation_models as sm\n",
    "import tensorflow as tf\n",
    "from focal_loss import BinaryFocalLoss,sparse_categorical_focal_loss\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.ndimage.filters import generic_filter\n",
    "\n",
    "# Input Image Dir\n",
    "image_dir = \"train/images/\"\n",
    "image_target_dir = \"train/targets/\"\n",
    "\n",
    "# Output Dir\n",
    "output_image_dir = \"blurred_images/train/images/\"\n",
    "output_target_dir = \"blurred_images/train/targets/\"\n",
    "\n",
    "# Noise Parameters \n",
    "\n",
    "gaussian_stddev = 0.35\n",
    "salt_pepper_probability = 0.002\n",
    "\n",
    "# Majority Filter Parameters \n",
    "\n",
    "perform_majority_filtering = 1\n",
    "neighborhood_size = (3, 3)  # 3x3 neighborhood\n",
    "threshold = 8  # Threshold value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1fc55",
   "metadata": {},
   "source": [
    "## Visualize Noisy Images - ( Optional - For Visualization )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83b92a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all image files in the directory\n",
    "# image_files = [f for f in os.listdir(image_dir) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "# indx = random.randint(0, len(image_files))\n",
    "# image_file = image_files[indx]\n",
    "\n",
    "\n",
    "# # Read the image\n",
    "# image_path = os.path.join(image_dir, image_file)\n",
    "# image = cv2.imread(image_path)\n",
    " \n",
    "# # Add Gaussian noise\n",
    "# mean = 0\n",
    "# stddev = gaussian_stddev\n",
    "# gaussian_noise = np.random.normal(mean, stddev, image.shape).astype(np.uint8)\n",
    "# noisy_image_gaussian = cv2.add(image, gaussian_noise)\n",
    "\n",
    "# # Add salt and pepper noise\n",
    "# salt_pepper_noise = np.zeros(image.shape, np.uint8)\n",
    "# probability = salt_pepper_probability\n",
    "# salt = np.where(np.random.rand(*image.shape[:2]) < probability)\n",
    "# pepper = np.where(np.random.rand(*image.shape[:2]) < probability)\n",
    "# salt_pepper_noise[salt] = 255\n",
    "# salt_pepper_noise[pepper] = 0\n",
    "# noisy_image_salt_pepper = cv2.add(image, salt_pepper_noise)\n",
    "\n",
    "# cv2.imshow(\"Original Image\", image)\n",
    "# cv2.imshow(\"Noisy Image (Salt&Pepper)\", noisy_image_salt_pepper)\n",
    "# cv2.imshow(\"Noisy Image (Gaussian)\", noisy_image_gaussian)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea08694",
   "metadata": {},
   "source": [
    "## Defining Basic Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03dbbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_filter(mask, neighborhood_size, threshold):\n",
    "    def majority_value(arr):\n",
    "        unique_classes, counts = np.unique(arr, return_counts=True, axis=0)\n",
    "        max_count = np.max(counts)\n",
    "        if max_count >= threshold:  # Check if the max count crosses the threshold\n",
    "            majority_class = unique_classes[np.argmax(counts)]\n",
    "            return majority_class\n",
    "        else:\n",
    "            return arr[int(len(arr)/2)]  # Keep the original value\n",
    "\n",
    "    filtered_mask = np.zeros_like(mask)\n",
    "\n",
    "    for channel in range(mask.shape[2]):\n",
    "        channel_filtered = generic_filter(mask[:, :, channel], majority_value, size=neighborhood_size)\n",
    "        filtered_mask[:, :, channel] = channel_filtered\n",
    "\n",
    "    return filtered_mask\n",
    "\n",
    "def bin_images(image):\n",
    "    # Convert pixel values less than 2 to 0 and values greater than or equal to 2 to 1\n",
    "    image[image < 2] = 0\n",
    "    image[image >= 2] = 1\n",
    "    \n",
    "    return image\n",
    "    \n",
    "    \n",
    "def generate_noisy_images(input_dir,image_target_dir,\n",
    "                          output_image_dir,output_target_dir,\n",
    "                          gaussian_stddev,salt_pepper_probability,\n",
    "                         perform_majority_filtering,\n",
    "                         bin_mask = False,\n",
    "                         early_stopping = False):\n",
    "\n",
    "    # Create separate folders for different types of noise\n",
    "    output_dir_image = os.path.join('', output_image_dir)\n",
    "    output_dir_mask = os.path.join('', output_target_dir)\n",
    "\n",
    "\n",
    "    os.makedirs(output_dir_image, exist_ok=True)\n",
    "    os.makedirs(output_dir_mask, exist_ok=True)\n",
    "\n",
    "\n",
    "    # List all image files in the directory\n",
    "    image_files = [f for f in os.listdir(input_dir) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Read the image\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Read Mask\n",
    "        image_mask_path = os.path.join(image_target_dir, image_file[:-4] + '_target.png')\n",
    "        mask = cv2.imread(image_mask_path)\n",
    "        #mask = Image.open(image_mask_path)\n",
    "        if bin_mask:\n",
    "            mask = bin_images(mask)\n",
    "        \n",
    "        if perform_majority_filtering:\n",
    "            mask = majority_filter(mask, neighborhood_size, threshold)\n",
    "        \n",
    "        print(mask.shape)\n",
    "\n",
    "        # Add Gaussian noise\n",
    "        mean = 0\n",
    "        stddev = gaussian_stddev\n",
    "        gaussian_noise = np.random.normal(mean, stddev, image.shape).astype(np.uint8)\n",
    "        noisy_image_gaussian = cv2.add(image, gaussian_noise)\n",
    "\n",
    "        # Add salt and pepper noise\n",
    "        salt_pepper_noise = np.zeros(image.shape, np.uint8)\n",
    "        probability = salt_pepper_probability\n",
    "        salt = np.where(np.random.rand(*image.shape[:2]) < probability)\n",
    "        pepper = np.where(np.random.rand(*image.shape[:2]) < probability)\n",
    "        salt_pepper_noise[salt] = 255\n",
    "        salt_pepper_noise[pepper] = 0\n",
    "        noisy_image_salt_pepper = cv2.add(image, salt_pepper_noise)\n",
    "\n",
    "\n",
    "\n",
    "        # Save the noisy images in separate folders\n",
    "        output_path_gaussian = os.path.join(output_dir_image, image_file[:-4]+'_gaussian'+image_file[-4:])\n",
    "        output_path_salt_pepper = os.path.join(output_dir_image, image_file[:-4]+'_salt_pepper'+image_file[-4:])\n",
    "        output_path_original = os.path.join(output_dir_image, image_file)\n",
    "        \n",
    "        # Mask for different Noise\n",
    "        output_path_gaussian_mask = os.path.join(output_dir_mask, image_file[:-4]+'_gaussian_target'+image_file[-4:])\n",
    "        output_path_salt_pepper_mask = os.path.join(output_dir_mask, image_file[:-4]+'_salt_pepper_target'+image_file[-4:])\n",
    "        output_path_original_mask = os.path.join(output_dir_mask, image_file[:-4]+'_target'+image_file[-4:])\n",
    "        \n",
    "        # Saving\n",
    "        print(output_path_gaussian)\n",
    "        cv2.imwrite(output_path_gaussian, noisy_image_gaussian)\n",
    "        cv2.imwrite(output_path_salt_pepper, noisy_image_salt_pepper)\n",
    "        cv2.imwrite(output_path_original, image)\n",
    "        \n",
    "        print(output_path_gaussian_mask)\n",
    "        \n",
    "        cv2.imwrite(output_path_gaussian_mask,mask)\n",
    "        cv2.imwrite(output_path_salt_pepper_mask,mask)\n",
    "        cv2.imwrite(output_path_original_mask,mask)\n",
    "        \n",
    "#         mask.save(output_path_gaussian_mask)\n",
    "#         mask.save(output_path_salt_pepper_mask)\n",
    "#         mask.save(output_path_original_mask)\n",
    "        \n",
    "        if early_stopping:\n",
    "            break\n",
    "\n",
    "    print(\"Noisy images saved.\")\n",
    "\n",
    "\n",
    "def load_data(data_dir, resize=False):\n",
    "    # Set the paths to the directories containing the dataset\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'targets')\n",
    "\n",
    "    \n",
    "    # Set the input image dimensions\n",
    "    input_shape = (128, 128, 3)  # Adjust as needed\n",
    "\n",
    "    # Get the list of image filenames\n",
    "    image_filenames = os.listdir(images_dir)\n",
    "\n",
    "    # Batch size for loading images\n",
    "    batch_size = 32\n",
    "\n",
    "    # Prepare the data for training\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Process images in batches\n",
    "    for i in range(0, len(image_filenames), batch_size):\n",
    "        batch_filenames = image_filenames[i:i + batch_size]\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "\n",
    "        for filename in batch_filenames:\n",
    "            # Load pre and post-disaster images\n",
    "            pre_image_path = os.path.join(images_dir, filename)\n",
    "            pre_image = cv2.imread(pre_image_path)\n",
    "\n",
    "            # Load the corresponding label\n",
    "            label_filename = filename[:-4] + '_target.png'\n",
    "            label_path = os.path.join(labels_dir, label_filename)\n",
    "            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if resize:\n",
    "                # Resize the images to the desired shape\n",
    "                pre_image = cv2.resize(pre_image, (input_shape[0], input_shape[1]))\n",
    "                label = cv2.resize(label, (input_shape[0], input_shape[1]))\n",
    "\n",
    "            # Add the data to the batch\n",
    "            batch_X.append(pre_image)\n",
    "            batch_y.append(label)\n",
    "\n",
    "        # Convert the batch lists to numpy arrays\n",
    "        batch_X = np.array(batch_X)\n",
    "        print(len(batch_y))\n",
    "\n",
    "        # Check if all labels in the batch have the same shape\n",
    "        label_shapes = set([label.shape for label in batch_y])\n",
    "        if len(label_shapes) > 1:\n",
    "            raise ValueError(\"Labels in the batch have different shapes.\")\n",
    "\n",
    "        batch_y = np.array(batch_y)\n",
    "\n",
    "        # Add the batch data to the training set\n",
    "        X_train.append(batch_X)\n",
    "        y_train.append(batch_y)\n",
    "\n",
    "    # Concatenate the batches to obtain the final training data\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = np.concatenate(y_train)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33a74ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 3)\n",
      "blurred_images/train/images/hurricane-harvey_00000015_pre_disaster_gaussian.png\n",
      "blurred_images/train/targets/hurricane-harvey_00000015_pre_disaster_gaussian_target.png\n",
      "Noisy images saved.\n"
     ]
    }
   ],
   "source": [
    "# Generate Noisy Image Dataset - Skip of Already Created \n",
    "\n",
    "generate_noisy_images(image_dir,image_target_dir,\n",
    "                      output_image_dir,output_target_dir,\n",
    "                      gaussian_stddev,salt_pepper_probability,\n",
    "                     perform_majority_filtering = 1 ,\n",
    "                     bin_mask = True,\n",
    "                     early_stopping = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed0616",
   "metadata": {},
   "source": [
    "### Testing - No Need to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbf228fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_target_dir = \"train/targets/\"\n",
    "# output_dir_mask = os.path.join('', output_target_dir)\n",
    "\n",
    "# # List all image files in the directory\n",
    "# image_files = [f for f in os.listdir(output_dir_mask) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "# max_pixels = 0\n",
    "# max_image_path = ''\n",
    "\n",
    "# for image_file in image_files:\n",
    "#     # Read the image\n",
    "#     image_path = os.path.join(output_dir_mask, image_file)\n",
    "#     image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#     # Count the pixels with value 2 and above\n",
    "#     if 2 in np.unique(image):\n",
    "#         print(image_path)\n",
    "#         print(np.unique(image))\n",
    "#         print(\"-------------------\")\n",
    "\n",
    "\n",
    "# #print(\"Image with the maximum number of pixels (value >= 2):\", max_image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ced88f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output_target_dir = \"train/targets/\"\n",
    "# # output_dir_mask = os.path.join('', output_target_dir)\n",
    "\n",
    "# # # List all image files in the directory\n",
    "# # image_files = [f for f in os.listdir(output_dir_mask) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "# # max_pixels = 0\n",
    "# # max_image_path = ''\n",
    "\n",
    "# # for image_file in image_files:\n",
    "# #     # Read the image\n",
    "# #     image_path = os.path.join(output_dir_mask, image_file)\n",
    "# #     image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# #     # Count the pixels with value 2 and above\n",
    "# #     count = np.sum(image >= 2)\n",
    "\n",
    "# #     # Update the maximum pixel count and image path if necessary\n",
    "# #     if count > max_pixels:\n",
    "# #         max_pixels = count\n",
    "# #         max_image_path = image_path\n",
    "\n",
    "# # print(\"Image with the maximum number of pixels (value >= 2):\", max_image_path)\n",
    "\n",
    "# ## --- >train/targets/palu-tsunami_00000118_post_disaster_target.png\n",
    "\n",
    "\n",
    "# # Read the image\n",
    "# def bin_images(image):\n",
    "#     # Convert pixel values less than 2 to 0 and values greater than or equal to 2 to 1\n",
    "#     image[image < 2] = 0\n",
    "#     image[image >= 2] = 1\n",
    "    \n",
    "#     return image\n",
    "\n",
    "# image = cv2.imread('train/images/palu-tsunami_00000118_post_disaster.png')\n",
    "# image_mask = cv2.imread('train/targets/palu-tsunami_00000118_post_disaster_target.png',cv2.IMREAD_GRAYSCALE)\n",
    "# binned_mask = bin_images(image_mask)\n",
    "\n",
    "\n",
    "# cv2.imshow(\"Original Image\", image)\n",
    "# cv2.imshow(\"Original Segment\", cv2.bitwise_and(image, image, mask=image_mask))\n",
    "# cv2.imshow(\"Binned Segment\", cv2.bitwise_and(image, image, mask=binned_mask))\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c6af9",
   "metadata": {},
   "source": [
    "## Loading Model Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6c03b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# BACKBONE = 'resnet50'\n",
    "\n",
    "# (X,y) = load_data('blurred_images/train',resize = False)\n",
    "\n",
    "# from keras.utils import normalize\n",
    "# from segmentation_models import get_preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# y = np.expand_dims(y, axis=3)\n",
    "\n",
    "# n_classes = 2\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# train_masks_cat = to_categorical(y, num_classes=n_classes)\n",
    "# y_train_cat = train_masks_cat.reshape((y.shape[0], y.shape[1], y.shape[2], n_classes))\n",
    "\n",
    "# X_train, X_val, y_train, y_val  = train_test_split(X, y_train_cat, test_size=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040750b",
   "metadata": {},
   "source": [
    "## Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afe136cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images belonging to 1 classes.\n",
      "Found 3 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train \n",
    "    train_images \n",
    "                train\n",
    "    train_masks\n",
    "                train\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the directories for images and masks\n",
    "train_images_dir = \"blurred_images/train/train_images\"\n",
    "train_masks_dir = \"blurred_images/train/train_masks\"\n",
    "\n",
    "seed=24\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Set up the image data generator for training data\n",
    "image_datagen = ImageDataGenerator()\n",
    "mask_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create the generators for training data\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    train_images_dir,\n",
    "    class_mode=None,\n",
    "    batch_size = batch_size,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    train_masks_dir,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=None,\n",
    "    batch_size = batch_size,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Combine the image and mask generators\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "num_train_imgs = len(os.listdir('blurred_images/train/train_images'))\n",
    "steps_per_epoch = num_train_imgs //batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3afdfc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "/var/folders/3y/1js5b25s1b1dk8g7drn7_g8h0000gn/T/ipykernel_16143/2951612275.py:34: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,epochs=400,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 19:03:03.121038: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 1.1033 - iou_score: 5.2183e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 1.1033 - iou_score: 5.2183e-10 - precision: 0.0000e+00\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0988 - iou_score: 5.4420e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 292ms/step - loss: 1.0988 - iou_score: 5.4420e-10 - precision: 0.0000e+00\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0945 - iou_score: 5.6714e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 239ms/step - loss: 1.0945 - iou_score: 5.6714e-10 - precision: 0.0000e+00\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0905 - iou_score: 5.9037e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 262ms/step - loss: 1.0905 - iou_score: 5.9037e-10 - precision: 0.0000e+00\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0869 - iou_score: 6.1398e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0869 - iou_score: 6.1398e-10 - precision: 0.0000e+00\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0834 - iou_score: 6.3813e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 276ms/step - loss: 1.0834 - iou_score: 6.3813e-10 - precision: 0.0000e+00\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0802 - iou_score: 6.6268e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 239ms/step - loss: 1.0802 - iou_score: 6.6268e-10 - precision: 0.0000e+00\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0771 - iou_score: 6.8781e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 252ms/step - loss: 1.0771 - iou_score: 6.8781e-10 - precision: 0.0000e+00\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0742 - iou_score: 7.1353e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 236ms/step - loss: 1.0742 - iou_score: 7.1353e-10 - precision: 0.0000e+00\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0715 - iou_score: 7.3976e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0715 - iou_score: 7.3976e-10 - precision: 0.0000e+00\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0689 - iou_score: 7.6643e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 304ms/step - loss: 1.0689 - iou_score: 7.6643e-10 - precision: 0.0000e+00\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0665 - iou_score: 7.9342e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 237ms/step - loss: 1.0665 - iou_score: 7.9342e-10 - precision: 0.0000e+00\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0642 - iou_score: 8.2093e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 232ms/step - loss: 1.0642 - iou_score: 8.2093e-10 - precision: 0.0000e+00\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0620 - iou_score: 8.4886e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 237ms/step - loss: 1.0620 - iou_score: 8.4886e-10 - precision: 0.0000e+00\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0599 - iou_score: 8.7715e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0599 - iou_score: 8.7715e-10 - precision: 0.0000e+00\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0580 - iou_score: 9.0591e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 285ms/step - loss: 1.0580 - iou_score: 9.0591e-10 - precision: 0.0000e+00\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0561 - iou_score: 9.3521e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step - loss: 1.0561 - iou_score: 9.3521e-10 - precision: 0.0000e+00\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0543 - iou_score: 9.6513e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 236ms/step - loss: 1.0543 - iou_score: 9.6513e-10 - precision: 0.0000e+00\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0526 - iou_score: 9.9561e-10 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 233ms/step - loss: 1.0526 - iou_score: 9.9561e-10 - precision: 0.0000e+00\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0510 - iou_score: 1.0264e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0510 - iou_score: 1.0264e-09 - precision: 0.0000e+00\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0494 - iou_score: 1.0575e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 264ms/step - loss: 1.0494 - iou_score: 1.0575e-09 - precision: 0.0000e+00\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0480 - iou_score: 1.0889e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 236ms/step - loss: 1.0480 - iou_score: 1.0889e-09 - precision: 0.0000e+00\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0466 - iou_score: 1.1206e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step - loss: 1.0466 - iou_score: 1.1206e-09 - precision: 0.0000e+00\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0452 - iou_score: 1.1526e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 233ms/step - loss: 1.0452 - iou_score: 1.1526e-09 - precision: 0.0000e+00\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0440 - iou_score: 1.1849e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0440 - iou_score: 1.1849e-09 - precision: 0.0000e+00\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0428 - iou_score: 1.2174e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 255ms/step - loss: 1.0428 - iou_score: 1.2174e-09 - precision: 0.0000e+00\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0416 - iou_score: 1.2501e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 234ms/step - loss: 1.0416 - iou_score: 1.2501e-09 - precision: 0.0000e+00\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0405 - iou_score: 1.2831e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 234ms/step - loss: 1.0405 - iou_score: 1.2831e-09 - precision: 0.0000e+00\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0395 - iou_score: 1.3164e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 282ms/step - loss: 1.0395 - iou_score: 1.3164e-09 - precision: 0.0000e+00\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0385 - iou_score: 1.3499e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0385 - iou_score: 1.3499e-09 - precision: 0.0000e+00\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0375 - iou_score: 1.3838e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step - loss: 1.0375 - iou_score: 1.3838e-09 - precision: 0.0000e+00\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0366 - iou_score: 1.4179e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step - loss: 1.0366 - iou_score: 1.4179e-09 - precision: 0.0000e+00\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0357 - iou_score: 1.4523e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step - loss: 1.0357 - iou_score: 1.4523e-09 - precision: 0.0000e+00\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0349 - iou_score: 1.4870e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 233ms/step - loss: 1.0349 - iou_score: 1.4870e-09 - precision: 0.0000e+00\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0340 - iou_score: 1.5220e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0340 - iou_score: 1.5220e-09 - precision: 0.0000e+00\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0333 - iou_score: 1.5573e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 331ms/step - loss: 1.0333 - iou_score: 1.5573e-09 - precision: 0.0000e+00\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0325 - iou_score: 1.5929e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 234ms/step - loss: 1.0325 - iou_score: 1.5929e-09 - precision: 0.0000e+00\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0318 - iou_score: 1.6289e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 233ms/step - loss: 1.0318 - iou_score: 1.6289e-09 - precision: 0.0000e+00\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0311 - iou_score: 1.6653e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 232ms/step - loss: 1.0311 - iou_score: 1.6653e-09 - precision: 0.0000e+00\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0304 - iou_score: 1.7019e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: saving model to training_weights/resnet50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0304 - iou_score: 1.7019e-09 - precision: 0.0000e+00\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0297 - iou_score: 1.7387e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 387ms/step - loss: 1.0297 - iou_score: 1.7387e-09 - precision: 0.0000e+00\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0291 - iou_score: 1.7759e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 234ms/step - loss: 1.0291 - iou_score: 1.7759e-09 - precision: 0.0000e+00\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0285 - iou_score: 1.8132e-09 - precision: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,iou_score,precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step - loss: 1.0285 - iou_score: 1.8132e-09 - precision: 0.0000e+00\n",
      "Epoch 44/400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(weights)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcp_save_weight\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/engine/training.py:2810\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \n\u001b[1;32m   2800\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2806\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2808\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[0;32m-> 2810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2812\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/engine/training.py:1748\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1746\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1747\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1748\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/utils/tf_utils.py:694\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/tensorflow/python/util/nest.py:624\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:1054\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1054\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/keras/src/utils/tf_utils.py:687\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 687\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "from segmentation_models.metrics import iou_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the pretrained model\n",
    "BACKBONE = 'resnet50'\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=50)\n",
    "\n",
    "checkpoint_path = \"training_weights/resnet50/imcp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_save_weight = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,verbose=1,save_weights_only= True,period = 5)\n",
    "\n",
    "\n",
    "# Set the input image dimensions\n",
    "input_shape = (1024, 1024, 3)\n",
    "\n",
    "model = sm.Unet(BACKBONE,encoder_weights='imagenet', input_shape=input_shape,classes=1)\n",
    "\n",
    "\n",
    "# Compile the model with sparse categorical cross-entropy loss\n",
    "# Define the optimizer and compile the model with the loss and metrics\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=bce_jaccard_loss, metrics=[iou_score,tf.keras.metrics.Precision()])\n",
    "\n",
    "weights = tf.train.latest_checkpoint('training_weights/')\n",
    "if  weights:\n",
    "    model.load_weights(weights)\n",
    "    \n",
    "\n",
    "# Train the model\n",
    "history = model.fit_generator(train_generator,epochs=400,\n",
    "                    steps_per_epoch=steps_per_epoch,callbacks=[early_stopping_cb,cp_save_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016472a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee649990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3ed87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
